# ZioYou-LLM의 기술적 원리 [Vector, Dimension, MoE ]
# ZioYou-Conf
( 2026년 1월 2일 )  
LLM(Large Language Model) 핵심 구조와 동작 원리를 중심으로 정리한 학습 기록

<hr>

<h2>정리 내용</h2>

이번 정리를 통해 가장 중요하게 느낀 부분은 <b>LLM이 단순한 텍스트 생성기가 아니라, 고차원 수학 구조 위에서 동작하는 확률 모델</b>이라는 점이었다.<br>
우리가 입력하는 문장 하나하나는 모델 내부에서 <b>토큰(Token)</b> 단위로 분해되고,<br>
각 토큰은 <b>벡터(Vector)</b>라는 수치적 표현으로 변환되어 처리된다.<br>
이 벡터 안에 포함된 숫자의 개수, 즉 <i>차원(Dimension)</i>이 높을수록 단어의 의미와 맥락을 더 정밀하게 담을 수 있다.<br>
<br>

또한 LLM의 성능을 결정짓는 중요한 요소로 <b>모델 파라미터(Parameter)</b>와 <b>레이어(Layer)</b> 구조가 있었다.<br>
수십억 개의 파라미터는 단어 간의 관계를 학습한 가중치이며,<br>
여러 층을 거치면서 단순한 패턴 인식에서 복합적인 추론으로 점점 고도화된다.<br>
이 모든 과정은 <i>행렬 연산(Matrix Calculation)</i>으로 이루어지며,<br>
GPU가 필수적인 이유도 이 대규모 병렬 연산 때문이라는 점이 명확해졌다.<br>
<br>

모델을 실제 환경에서 효율적으로 사용하기 위한 기술로는 <b>양자화(Quantization)</b>가 핵심이었다.<br>
FP16이나 BF16 같은 기본 정밀도를 INT8, INT4 수준으로 낮추면<br>
모델 용량과 메모리 사용량이 크게 줄어들어 개인 PC나 CPU 환경에서도 구동이 가능해진다.<br>
다만 이 과정에서 미세한 성능 저하가 발생할 수 있다는 점은 트레이드오프로 이해해야 한다.<br>
<br>

추론 방식의 측면에서는 <b>Instruct 모델</b>과 <b>Think(CoT) 모델</b>의 차이가 특히 중요하게 다가왔다.<br>
Instruct 모델이 즉각적인 출력에 초점을 둔다면,<br>
Think 모델은 문제를 여러 단계로 나누어 내부적으로 추론을 수행한다.<br>
<br>

시스템 관점에서 인상 깊었던 기술은 <b>KV Cache</b>였다.<br>
이전 대화의 Key-Value를 메모리에 저장함으로써<br>
컨텍스트가 길어져도 동일한 계산을 반복하지 않게 해주며,<br>
응답 속도와 비용 절감에 직접적인 영향을 준다.<br>
또한 CUDA, ROCm 같은 하드웨어 가속 라이브러리가<br>
LLM 생태계의 기반 인프라라는 점도 명확히 이해할 수 있었다.<br>
<br>

마지막으로 <b>MoE(Mixture of Experts)</b> 구조와 미래 트렌드가 강하게 인상에 남았다.<br>
모든 파라미터를 항상 사용하는 방식이 아니라,<br>
질문에 맞는 일부 전문가 모델만 활성화하는 구조는<br>
확장성과 효율성을 동시에 잡기 위한 핵심 설계라는 생각이 들었다.<br>
<br>

<hr>

<h2>느낀 점</h2>

이번 정리를 하면서 LLM을 단순히 “큰 모델”로만 이해하는 시각에서 벗어나게 되었다.<br>
LLM은 <i>토큰화 → 벡터화 → 고차원 행렬 연산 → 확률적 추론</i>이라는 매우 정교한 흐름 위에서 동작하고 있었고,<br>
각 단계마다 성능과 비용을 좌우하는 기술적 선택이 존재한다는 점이 명확해졌다.<br>
<br>

특히 양자화, KV Cache, MoE 같은 기술은<br>
이론이 아니라 실제 서비스 환경에서 LLM을 “쓸 수 있게 만드는 기술”이라는 점에서 의미가 컸다.<br>
앞으로 LLM을 다룰 때는 단순한 프롬프트 엔지니어링을 넘어서,<br>
모델 구조·연산 방식·하드웨어 제약까지 함께 고려해야겠다는 생각이 들었다.<br>
<br>
